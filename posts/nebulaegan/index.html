<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generating nebulae images with GANs | Tan Nian Wei</title><meta name=keywords content="python,GAN,deep-learning"><meta name=description content="End-to-end from data scraping, to model training, to deployment as web app"><meta name=author content><link rel=canonical href=https://tnwei.github.io/posts/nebulaegan/><link crossorigin=anonymous href=/assets/css/stylesheet.min.3f18978ad811c7ac935d52e17228171d2a4141b9d8c9c35a5899222d655e34b8.css integrity="sha256-PxiXitgRx6yTXVLhcigXHSpBQbnYycNaWJkiLWVeNLg=" rel="preload stylesheet" as=style><link rel=icon href=https://tnwei.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tnwei.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tnwei.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tnwei.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tnwei.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.111.0"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-TCJ4MGEGS8"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-TCJ4MGEGS8")</script><style>figcaption{text-align:center}</style><meta property="og:title" content="Generating nebulae images with GANs"><meta property="og:description" content="End-to-end from data scraping, to model training, to deployment as web app"><meta property="og:type" content="article"><meta property="og:url" content="https://tnwei.github.io/posts/nebulaegan/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-26T19:16:00+08:00"><meta property="article:modified_time" content="2022-02-26T19:16:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Generating nebulae images with GANs"><meta name=twitter:description content="End-to-end from data scraping, to model training, to deployment as web app"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tnwei.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Generating nebulae images with GANs","item":"https://tnwei.github.io/posts/nebulaegan/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generating nebulae images with GANs","name":"Generating nebulae images with GANs","description":"End-to-end from data scraping, to model training, to deployment as web app","keywords":["python","GAN","deep-learning"],"articleBody":"I recently rigged up These Nebulae Do Not Exist. The website shows GAN-generated images of nebulae every time the page is refreshed:\nScreenshot of These Nebulae Do Not Exist\nIn this post, I cover how I assembled it end-to-end, from data scraping, to model training, and finally deploying it as a web app using free services only.\nIntroduction Nebulae are my favourite astronomical phenomena. Distant clouds of stardust arrayed like cosmological flowers against the void of the universe, made visible to us thanks to the stupefying resolving power of our space observatories. Inspired by the launch of the James Webb Space Telescope, and the GAN-powered realistic face generator at thispersondoesnotexist.com, I thought of looking at what GANs can do to generate images of nebulae.\nExisting work Screening for existing work, I found:\npearsonkyle/Neural-Nebula which uses DCGAN to generate 128x128 nebula pictures. jacobbieker/AstroGAN which contains code for a spiral galaxy generator, elliptical galaxy generator, and a 128x128 image generator trained on Hubble space telescope images. Both repos were last updated in Apr and May 2019 respectively, back before high resolution image synthesis with GANs became commonplace.\nData Data was scraped from publicly available images hosted by space agencies, mostly from NASA, ESA and ESO. Selenium was used to load search results from their image archives, then BeautifulSoup was used to pick out download links from individual image pages.\nWhen skimming through the scraped images, I noticed that the images scraped have varying size and perspecive. Some images show a segment of space where the nebulae is a small object instead of being the focal point of the image. Some images are much longer than they are wide. Some images have pretty high resolution and need to be downsized (e.g. image width and height near ~8000 pixels)\nI ended up manually inspecting images in the dataset one by one and preprocessed them as necessary.\nImages that didn’t fit what I wanted were discarded. Photos too large to work with are downsized using GIMP, with image sharpening masks applied in moderation. Images that had a lot of empty space were cropped into smaller individual images that focused on interesting nebulae features. Composite images that have image-stitching artifacts were cropped to remove blank regions in the image (see example below). If left unattended, generators eventually learnt to produce images with similar stitching artifacts to fool discriminators. Example image with stitching artifacts. Full HST WFPC2 image of Trifid Nebula. Source: ESA/Hubble\nBy the time I was done, I had a vetted dataset of ~2k images. At this point, the images have a diverse range of resolutions and aspect ratios thanks to the preprocessing. However, most GANs only work with generating square images. Conventionally, one would simply resize the images to the target resolution, but I opted instead to sample random crops at 512x512 resolution to prevent introducing distortions.\nTraining I adopted a lightweight GAN architecture from Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis by Liu et al, implemented in lucidrains/lightweight-gan by the same person who made thispersondoesnotexist.com. Paraphrasing the paper’s summary, this lightweight GAN’s main value proposition is its low computational and data sample requirements, enabling high resolution GAN models to be trained on consumer GPUs within hours, using a small amount of images. This is great as I plan to rely on my personal workstation for training.\nI experimented with the training configuration for a couple of weeks. Overall I was pleased with the results; the images generated were sufficiently realistic and exhibited a diverse range of modalities. Examples below:\nExample GAN output 1\nExample GAN output 2\nHowever, every now and then, the generated images exhibit texture artifacts that proved to be difficult to remove despite further experimentation.\nExamples of GAN texture artifacts. You might need to open the image in a new tab and zoom in to see it\nI eventually concluded that the lightweight GAN architecture has reached its limits, and I’ve reaped all the benefits for something that trains this fast. Using GaParmar/clean-fid, the final model has an FID score 1 of 52.86. For context, Liu et al’s model scored 52.47 FID when trained on 2k images of nature photographs, which places us in a similar ballpark for performance (see table reproduced below).\nFID scores of Liu et al’s architecture with varying amounts of data, reproduced from their paper. Highlight emphasis mine.\n(Not) experimenting with StyleGAN (yet) At time of writing, the StyleGAN series of models is one of the most well known architectures for generating high fidelity images. According to paperswithcode.com, StyleGAN2 ranks near the top in benchmarks for unconditional image generation and is pretty close to state of the art. The latest iteration is StyleGAN3 released in Oct 2021, which I eagerly adopted to try for this dataset.\nSince I was using my personal workstation, model training is progressing rather slowly. According to StyleGAN3 documentation, fully training a model on the FFHQ dataset at 1024x1024 resolution require 5 days 8 hours 2 on 8 V100 GPUs. Given that a p2.8xlarge instance on AWS EC2 with that many GPUs costs 7.2 USD/hr, a single training run on a VM would have cost me ~900 USD. And that’s just base StyleGAN2 without the additional improvements from StyleGAN3; training StyleGAN3-T or StyleGAN3-R would have cost 1.5k USD 3 and 1.8k USD 4!\nGiven that StyleGAN uses a monstrous amount of compute that I’m not willing to pay for, I’m happy to slowly chip at it on a single GPU over time. This arrangement however places architectural improvements out of immediate reach; practically speaking I would only see returns for experimenting with StyleGAN3 after doing another (or another two) side projects.\nI wasn’t keen on waiting that long, and thus as dictated by the project management triangle, once cost and time is prioritized, scope has to give way. The current model has some flaws, but it makes sense to move ahead with deployment and revisit this in the future.\nModel export My first thought was to export the trained generator as an ONNX model. Conventionally, the main use case for exporting to ONNX is to allow model inferencing on a wide variety of languages via the ONNX runtime. In this case, exporting to ONNX is useful because the ONNX runtime in Python only requires protobuf and numpy, allowing a lean, Pytorch-less environment to be used in deployment.\nONNX export relies on first JIT-serializing the model to Torchscript, an intermediate representation of a Pytorch model. This can be done via either scripting or tracing. In scripting, model logic is faithfully replicated from source code. In tracing, a dummy input is forward propagated through the model. The operations that took place are then observed and recorded by Pytorch. Scripting tends to be preferred since it fully represents the model’s control flow, while tracing only records the control flow path taken when using the dummy input. However, scripting requires converting each model operation to a Torchscript counterpart; models with unsupported operations cannot be scripted. (I recommend this blogpost for further reading on this topic.)\nModel export went smoothly using torch.onnx.export. I wasn’t able to first convert the model to Torchscript using scripting, so I exported to ONNX using tracing instead. Upon comparison with images generated by the original model, the ONNX-exported model generated images are slightly different from the original model. In other words, the exported model was slightly different from the original model. There were dynamic control flow statements that wasn’t adequately represented using tracing. I wouldn’t mind if the different images looked better, but I prefer the output of the original model.\nComparison of images generated by ONNX-exported model (top) vs original Pytorch model (bottom)\nI modified the source code to remove blockers for scripting. They were small logical changes like replacing list unpacking with explicit indexing, and replacing lambda functions. I stopped when Pytorch complained about the use of einops.rearrange, which accepts an arbitrary amount of function arguments:\nNotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults: File \"/home/tnwei/miniconda3/envs/gan/lib/python3.9/site-packages/einops/ einops.py\", line 393 def rearrange(tensor, pattern: str, **axes_lengths): ~~~~~~~~~~~~~ \u003c--- HERE \"\"\" einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. einops is a useful package that allows expressing complex tensor manipulation in declarative Einstein notation. At time of writing, einops functions have yet to support scripting to Torchscript according to this github issue. Enabling scripting for the trained generator would require me to replace all calls to einops with equivalent logic. Under any circumstance but this, I’m a huge fan of the package. But in this scenario, untangling elegant Einstein notation like (b h) (x y) d -\u003e b (h d) x y into corresponding torch.reshape’s and torch.permute’s is too much for a Saturday afternoon.\nI decided to just extract the trained generator’s state_dict and its source code for inferencing using Pytorch.\nDeployment The end product of this work is envisioned to be a simple website that shows a new generated image upon page refresh. I went ahead and cobbled together a simple FastAPI app packaged in a Docker container that serves a generated image in a static HTML landing page. On my workstation with a 6-core Ryzen 5 3600 CPU, generating a new image takes ~1 second on average.\nI imagine someone visiting the site would not want to wait long to see another image when refreshing the page. Thus for good user experience, I need to reduce page refresh latency to at least a few hundred milliseconds. Thanks to the extra moments thinking about cost (see previous section), I’ve also made up my mind to only use free resources for this side project. In other words, deployment options are limited to cloud free tier limits. Thus, renting powerful compute is out of the question, GPU-accelerated or not. Latency is an issue that will need to be resolved without scaling compute.\nIt eventually occured to me that the fastest way to generate an image is to load one that already exists. The website can load pre-generated images from object storage, which is routinely topped up with model inferencing running in background workers. Effectively, the wait time for generating images is hidden from the end user, as long as the store of pre-generated images is not fully depleted. If the pre-generated images run out, the user will need to wait for the backend to directly generate a new image, which honestly isn’t the end of the world for a side project like this. Sounds like a plan.\nMajor cloud providers are not keen on giving you an always-free-tier VM. For context, AWS only makes the t2-micro and t3-micro instances available for this purpose, they come with either 1 or 2 vCPUs and a measly 1GB RAM, and will be performance-throttled if you’re using them too much. By contrast, they are much more generous with their serverless services. Google Cloud Run and Azure Container Apps allow running containerized applications within an always-free-tier usage quota 5. Given that my web app doesn’t need to be stateful, a serverless architecture is fair game. Between GCP and Azure, I chose to go with Cloud Run since I’ve seen more mentions of it on the internet compared to Container Apps.\nThe complete application architecture diagram is as follows:\nThe existing FastAPI web app is split into a light frontend that only serves the HTML page, and a relatively heavy backend that only generates images. When called, the frontend serves the landing page using the oldest image from the bucket. Once served, a Cloud Function calls the backend to generate a new image to replace the image that was just displayed. If multiple calls to the frontend are made in quick succession, Cloud Run will start up additional backend instances to cater for the increase in load.\nIt takes an average of ~7 seconds for the backend to generate and return a new image. Thanks to how the application architecture is set up, the frontend only needs an average of 310 ms to load the image from object storage, which fits my requirements.\nThe final step for deployment is to set up a vanity URL redirecting to the Cloud Run frontend. Sticking to my guns on using free resources only, I used a cheeky iframe to embed the server-rendered front end in my static github.io personal site.\nConclusion and further thoughts You might be surprised to hear that throughout this project, dataset curation easily took the most time and manual effort. I think it was well worth the effort; I doubt GAN training would have went as smoothly if I just dumped the scraped output directly into model training.\nPersonally I’m not satisfied with the quality of the images generated; I think there is definitely room for improvement. I do plan to update the deployed model if further experimentation bears fruit.\nThis project stands on the shoulders of giants. Its building blocks exist thanks to the rapid advances of ML research, the rise of skilled ML research engineers implementing user-friendly training codebases, and the proliferation of cloud services. If we rewind the clock by a couple of years, it would not have been possible to complete this project with the level of effort I put into it. These good people have my gratitude.\nFID represents Fréchet inception distance, a metric for similarity between generated images and actual images. Lower is better. Wikipedia link ↩︎\nTime to fully train StyleGAN2: 17.55 s/k-imgs * 25000 k-imgs * 1.05 ~= 460k seconds ~= 5d 8h. ↩︎\nTime to fully StyleGAN3-T: 28.71 s/k-imgs * 25000 k-imgs * 1.05 ~= 754k s ~= 8d 17h. Times 7.2 USD/hr gives 1505 USD. ↩︎\nTime to fully train StyleGAN3-R: 34.12 s/k-imgs * 25000 k-imgs * 1.05 ~= 754k s ~= 10d 9h. Times 7.2 USD/hr gives 1793 USD. ↩︎\nI was surprised to find that AWS Fargate has no free-tier usage quota. ↩︎\n","wordCount":"2304","inLanguage":"en","datePublished":"2022-02-26T19:16:00+08:00","dateModified":"2022-02-26T19:16:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tnwei.github.io/posts/nebulaegan/"},"publisher":{"@type":"Organization","name":"Tan Nian Wei","logo":{"@type":"ImageObject","url":"https://tnwei.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://tnwei.github.io/ accesskey=h title="Tan Nian Wei (Alt + H)">Tan Nian Wei</a>
<span class=logo-switches></span></div><ul id=menu><li><a href=https://tnwei.github.io/writing/ title=Writing><span>Writing</span></a></li><li><a href="https://drive.google.com/file/d/1kWKGkamiCd7RZLssEBy3b-FT3r0jpwha/view?usp=sharing" title=Resume><span>Resume</span></a></li><li><a href=https://tnwei.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Generating nebulae images with GANs</h1><div class=post-meta><span title='2022-02-26 19:16:00 +0800 +0800'>Feb 26, 2022 Sat</span>&nbsp;·&nbsp;11 min</div></header><div class=post-content><p>I recently rigged up <a href=https://tnwei.github.io/thesenebulaedonotexist>These Nebulae Do Not Exist</a>. The website shows GAN-generated images of nebulae every time the page is refreshed:</p><figure><img loading=lazy src=/static/thesenebulaedonotexist/site-screenshot.jpeg alt="Site screenshot"><figcaption><p>Screenshot of These Nebulae Do Not Exist</p></figcaption></figure><p>In this post, I cover how I assembled it end-to-end, from data scraping, to model training, and finally deploying it as a web app using free services only.</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Nebulae are my favourite astronomical phenomena. Distant clouds of stardust arrayed like cosmological flowers against the void of the universe, made visible to us thanks to the stupefying resolving power of our space observatories. Inspired by the launch of the James Webb Space Telescope, and the GAN-powered realistic face generator at <a href=https://thispersondoesnotexist.com>thispersondoesnotexist.com</a>, I thought of looking at what GANs can do to generate images of nebulae.</p><h2 id=existing-work>Existing work<a hidden class=anchor aria-hidden=true href=#existing-work>#</a></h2><p>Screening for existing work, I found:</p><ul><li><a href=https://github.com/pearsonkyle/Neural-Nebula><code>pearsonkyle/Neural-Nebula</code></a> which uses DCGAN to generate 128x128 nebula pictures.</li><li><a href=https://github.com/jacobbieker/AstroGAN><code>jacobbieker/AstroGAN</code></a> which contains code for a spiral galaxy generator, elliptical galaxy generator, and a 128x128 image generator trained on Hubble space telescope images.</li></ul><p>Both repos were last updated in Apr and May 2019 respectively, back before high resolution image synthesis with GANs became commonplace.</p><h2 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h2><p>Data was scraped from publicly available images hosted by space agencies, mostly from NASA, ESA and ESO. Selenium was used to load search results from their image archives, then BeautifulSoup was used to pick out download links from individual image pages.</p><p>When skimming through the scraped images, I noticed that the images scraped have varying size and perspecive. Some images show a segment of space where the nebulae is a small object instead of being the focal point of the image. Some images are much longer than they are wide. Some images have pretty high resolution and need to be downsized (e.g. image width and height near ~8000 pixels)</p><p>I ended up manually inspecting images in the dataset one by one and preprocessed them as necessary.</p><ul><li>Images that didn&rsquo;t fit what I wanted were discarded.</li><li>Photos too large to work with are downsized using GIMP, with image sharpening masks applied in moderation.</li><li>Images that had a lot of empty space were cropped into smaller individual images that focused on interesting nebulae features.</li><li>Composite images that have image-stitching artifacts were cropped to remove blank regions in the image (see example below). If left unattended, generators eventually learnt to produce images with similar stitching artifacts to fool discriminators.</li></ul><figure><img loading=lazy src=/static/thesenebulaedonotexist/opo0417g-shrinked-shrinked.jpeg alt="Example photo with image stitching artifacts"><figcaption><p>Example image with stitching artifacts. <em>Full HST WFPC2 image of Trifid Nebula</em>. Source: ESA/Hubble</p></figcaption></figure><p>By the time I was done, I had a vetted dataset of ~2k images. At this point, the images have a diverse range of resolutions and aspect ratios thanks to the preprocessing. However, most GANs only work with generating square images. Conventionally, one would simply resize the images to the target resolution, but I opted instead to sample random crops at 512x512 resolution to prevent introducing distortions.</p><h2 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h2><p>I adopted a lightweight GAN architecture from <a href="https://openreview.net/forum?id=1Fqg133qRaI">Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis</a> by Liu et al, implemented in <a href=https://github.com/lucidrains/lightweight-gan>lucidrains/lightweight-gan</a> by the same person who made <a href=https://thispersondoesnotexist.com>thispersondoesnotexist.com</a>. Paraphrasing the paper&rsquo;s summary, this lightweight GAN&rsquo;s main value proposition is its low computational and data sample requirements, enabling high resolution GAN models to be trained on consumer GPUs within hours, using a small amount of images. This is great as I plan to rely on my personal workstation for training.</p><p>I experimented with the training configuration for a couple of weeks. Overall I was pleased with the results; the images generated were sufficiently realistic and exhibited a diverse range of modalities. Examples below:</p><p><figure><img loading=lazy src=/static/thesenebulaedonotexist/example.jpeg alt="Example GAN output 1"><figcaption><p>Example GAN output 1</p></figcaption></figure><figure><img loading=lazy src=/static/thesenebulaedonotexist/example2.jpeg alt="Example GAN output 2"><figcaption><p>Example GAN output 2</p></figcaption></figure></p><p>However, every now and then, the generated images exhibit texture artifacts that proved to be difficult to remove despite further experimentation.</p><figure><img loading=lazy src=/static/thesenebulaedonotexist/example-texture-artifacts.png alt="Image of GAN texture artifacts"><figcaption><p>Examples of GAN texture artifacts. You might need to open the image in a new tab and zoom in to see it</p></figcaption></figure><p>I eventually concluded that the lightweight GAN architecture has reached its limits, and I&rsquo;ve reaped all the benefits for something that trains this fast. Using <a href=https://github.com/GaParmar/clean-fid>GaParmar/clean-fid</a>, the final model has an FID score <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> of 52.86. For context, Liu et al&rsquo;s model scored 52.47 FID when trained on 2k images of nature photographs, which places us in a similar ballpark for performance (see table reproduced below).</p><figure><img loading=lazy src=/static/thesenebulaedonotexist/fid-excerpt.png alt="Table reproduced from paper"><figcaption><p>FID scores of Liu et al&rsquo;s architecture with varying amounts of data, reproduced from their paper. Highlight emphasis mine.</p></figcaption></figure><h2 id=not-experimenting-with-stylegan-yet>(Not) experimenting with StyleGAN (yet)<a hidden class=anchor aria-hidden=true href=#not-experimenting-with-stylegan-yet>#</a></h2><p>At time of writing, the StyleGAN series of models is one of the most well known architectures for generating high fidelity images. According to <a href=https://paperswithcode.com/task/image-generation>paperswithcode.com</a>, StyleGAN2 ranks near the top in benchmarks for unconditional image generation and is pretty close to state of the art. The latest iteration is StyleGAN3 released in Oct 2021, which I eagerly adopted to try for this dataset.</p><p>Since I was using my personal workstation, model training is progressing rather slowly. According to <a href=https://github.com/NVlabs/stylegan3/blob/main/docs/configs.md>StyleGAN3 documentation</a>, fully training a model on the FFHQ dataset at 1024x1024 resolution require 5 days 8 hours <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> on 8 V100 GPUs. Given that a <code>p2.8xlarge</code> instance on AWS EC2 with that many GPUs costs 7.2 USD/hr, a single training run on a VM would have cost me ~900 USD. And that&rsquo;s just base StyleGAN2 without the additional improvements from StyleGAN3; training StyleGAN3-T or StyleGAN3-R would have cost 1.5k USD <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> and 1.8k USD <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>!</p><p>Given that StyleGAN uses a monstrous amount of compute that I&rsquo;m not willing to pay for, I&rsquo;m happy to slowly chip at it on a single GPU over time. This arrangement however places architectural improvements out of immediate reach; practically speaking I would only see returns for experimenting with StyleGAN3 after doing another (or another <em>two</em>) side projects.</p><p>I wasn&rsquo;t keen on waiting that long, and thus as dictated by the <a href=https://en.wikipedia.org/wiki/Project_management_triangle>project management triangle,</a> once cost and time is prioritized, scope has to give way. The current model has some flaws, but it makes sense to move ahead with deployment and revisit this in the future.</p><h2 id=model-export>Model export<a hidden class=anchor aria-hidden=true href=#model-export>#</a></h2><p>My first thought was to export the trained generator as an <a href=https://onnx.ai/>ONNX model</a>. Conventionally, the main use case for exporting to ONNX is to allow model inferencing on a wide variety of languages via the ONNX runtime. In this case, exporting to ONNX is useful because the ONNX runtime in Python only requires protobuf and numpy, allowing a lean, Pytorch-less environment to be used in deployment.</p><p>ONNX export relies on first JIT-serializing the model to <a href=https://pytorch.org/docs/stable/jit.html>Torchscript</a>, an intermediate representation of a Pytorch model. This can be done via either scripting or tracing. In scripting, model logic is faithfully replicated from source code. In tracing, a dummy input is forward propagated through the model. The operations that took place are then observed and recorded by Pytorch. Scripting tends to be preferred since it fully represents the model&rsquo;s control flow, while tracing only records the control flow path taken when using the dummy input. However, scripting requires converting each model operation to a Torchscript counterpart; models with unsupported operations cannot be scripted. (I recommend <a href=https://paulbridger.com/posts/mastering-torchscript/#tracing-vs-scripting>this blogpost</a> for further reading on this topic.)</p><p>Model export went smoothly using <a href=https://pytorch.org/docs/stable/onnx.html><code>torch.onnx.export</code></a>. I wasn&rsquo;t able to first convert the model to Torchscript using scripting, so I exported to ONNX using tracing instead. Upon comparison with images generated by the original model, the ONNX-exported model generated images are slightly different from the original model. In other words, the exported model was slightly different from the original model. There were dynamic control flow statements that wasn&rsquo;t adequately represented using tracing. I wouldn&rsquo;t mind if the different images looked better, but I prefer the output of the original model.</p><figure><img loading=lazy src=/static/thesenebulaedonotexist/onnx-vs-torch-2.jpeg alt="Image comparing output of the ONNX-exported model and the original model in Pytorch "><figcaption><p>Comparison of images generated by ONNX-exported model (top) vs original Pytorch model (bottom)</p></figcaption></figure><p>I modified the source code to remove blockers for scripting. They were small logical changes like replacing list unpacking with explicit indexing, and replacing lambda functions. I stopped when Pytorch complained about the use of <code>einops.rearrange</code>, which accepts an arbitrary amount of function arguments:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>NotSupportedError: Compiled functions can&#39;t take variable number of arguments
</span></span><span style=display:flex><span>or use keyword-only arguments with defaults:
</span></span><span style=display:flex><span>  File &#34;/home/tnwei/miniconda3/envs/gan/lib/python3.9/site-packages/einops/
</span></span><span style=display:flex><span>  einops.py&#34;, line 393
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def rearrange(tensor, pattern: str, **axes_lengths):
</span></span><span style=display:flex><span>                                     ~~~~~~~~~~~~~ &lt;--- HERE
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    einops.rearrange is a reader-friendly smart element reordering for
</span></span><span style=display:flex><span>    multidimensional tensors.
</span></span></code></pre></div><p><a href=https://github.com/arogozhnikov/einops><code>einops</code></a> is a useful package that allows expressing complex tensor manipulation in declarative Einstein notation. At time of writing, <code>einops</code> functions have yet to support scripting to Torchscript according <a href=https://github.com/arogozhnikov/einops/issues/115>to this github issue</a>. Enabling scripting for the trained generator would require me to replace all calls to <code>einops</code> with equivalent logic. Under any circumstance but this, I&rsquo;m a huge fan of the package. But in this scenario, untangling elegant Einstein notation like <code>(b h) (x y) d -> b (h d) x y</code> into corresponding <code>torch.reshape</code>&rsquo;s and <code>torch.permute</code>&rsquo;s is too much for a Saturday afternoon.</p><p>I decided to just extract the trained generator&rsquo;s <code>state_dict</code> and its source code for inferencing using Pytorch.</p><h2 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h2><p>The end product of this work is envisioned to be a simple website that shows a new generated image upon page refresh. I went ahead and cobbled together a simple FastAPI app packaged in a Docker container that serves a generated image in a static HTML landing page. On my workstation with a 6-core Ryzen 5 3600 CPU, generating a new image takes ~1 second on average.</p><p>I imagine someone visiting the site would not want to wait long to see another image when refreshing the page. Thus for good user experience, I need to reduce page refresh latency to at least a few hundred milliseconds. Thanks to the extra moments thinking about cost (see <a href=#not-experimenting-with-stylegan-yet>previous section</a>), I&rsquo;ve also made up my mind to only use free resources for this side project. In other words, deployment options are limited to cloud free tier limits. Thus, renting powerful compute is out of the question, GPU-accelerated or not. Latency is an issue that will need to be resolved without scaling compute.</p><p>It eventually occured to me that the fastest way to generate an image is to load one that already exists. The website can load pre-generated images from object storage, which is routinely topped up with model inferencing running in background workers. Effectively, the wait time for generating images is hidden from the end user, as long as the store of pre-generated images is not fully depleted. If the pre-generated images run out, the user will need to wait for the backend to directly generate a new image, which honestly isn&rsquo;t the end of the world for a side project like this. Sounds like a plan.</p><p>Major cloud providers are not keen on giving you an always-free-tier VM. For context, AWS only makes the <code>t2-micro</code> and <code>t3-micro</code> instances available for this purpose, they come with either 1 or 2 vCPUs and a measly 1GB RAM, and will be performance-throttled if you&rsquo;re using them too much. By contrast, they are much more generous with their serverless services. Google Cloud Run and Azure Container Apps allow running containerized applications within an always-free-tier usage quota <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Given that my web app doesn&rsquo;t need to be stateful, a serverless architecture is fair game. Between GCP and Azure, I chose to go with Cloud Run since I&rsquo;ve seen more mentions of it on the internet compared to Container Apps.</p><p>The complete application architecture diagram is as follows:</p><figure><img loading=lazy src=/static/thesenebulaedonotexist/nebulaegan-arch.jpeg alt="Image of These Nebulae Do Not Exist's application architecture diagram"></figure><p>The existing FastAPI web app is split into a light frontend that only serves the HTML page, and a relatively heavy backend that only generates images. When called, the frontend serves the landing page using the oldest image from the bucket. Once served, a Cloud Function calls the backend to generate a new image to replace the image that was just displayed. If multiple calls to the frontend are made in quick succession, Cloud Run will start up additional backend instances to cater for the increase in load.</p><p>It takes an average of ~7 seconds for the backend to generate and return a new image. Thanks to how the application architecture is set up, the frontend only needs an average of 310 ms to load the image from object storage, which fits my requirements.</p><p>The final step for deployment is to set up a vanity URL redirecting to the Cloud Run frontend. Sticking to my guns on using free resources only, I used a cheeky iframe to embed the server-rendered front end in my static <code>github.io</code> personal site.</p><h2 id=conclusion-and-further-thoughts>Conclusion and further thoughts<a hidden class=anchor aria-hidden=true href=#conclusion-and-further-thoughts>#</a></h2><p>You might be surprised to hear that throughout this project, dataset curation easily took the most time and manual effort. I think it was well worth the effort; I doubt GAN training would have went as smoothly if I just dumped the scraped output directly into model training.</p><p>Personally I&rsquo;m not satisfied with the quality of the images generated; I think there is definitely room for improvement. I do plan to update the deployed model if further experimentation bears fruit.</p><p>This project stands on the shoulders of giants. Its building blocks exist thanks to the rapid advances of ML research, the rise of skilled ML research engineers implementing user-friendly training codebases, and the proliferation of cloud services. If we rewind the clock by a couple of years, it would not have been possible to complete this project with the level of effort I put into it. These good people have my gratitude.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>FID represents Fréchet inception distance, a metric for similarity between generated images and actual images. Lower is better. <a href=https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance>Wikipedia link</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Time to fully train StyleGAN2: 17.55 s/k-imgs * 25000 k-imgs * 1.05 ~= 460k seconds ~= 5d 8h.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Time to fully StyleGAN3-T: 28.71 s/k-imgs * 25000 k-imgs * 1.05 ~= 754k s ~= 8d 17h. Times 7.2 USD/hr gives 1505 USD.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Time to fully train StyleGAN3-R: 34.12 s/k-imgs * 25000 k-imgs * 1.05 ~= 754k s ~= 10d 9h. Times 7.2 USD/hr gives 1793 USD.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>I was surprised to find that AWS Fargate has no free-tier usage quota.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://tnwei.github.io/tags/python/>python</a></li><li><a href=https://tnwei.github.io/tags/gan/>GAN</a></li><li><a href=https://tnwei.github.io/tags/deep-learning/>deep-learning</a></li></ul></footer><script src=https://giscus.app/client.js data-repo=tnwei/tnwei.github.io data-repo-id=R_kgDOGmOfvQ data-category=Announcements data-category-id=DIC_kwDOGmOfvc4CAeit data-mapping=og:title data-reactions-enabled=0 data-emit-metadata=0 data-theme=light data-lang=en crossorigin=anonymous async></script><noscript></noscript></article></main><footer class=footer><span>&copy; 2024 <a href=https://tnwei.github.io/>Tan Nian Wei</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>