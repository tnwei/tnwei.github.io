<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Explaining negative R-squared | Tan Nian Wei</title><meta name=keywords content="statistics"><meta name=description content="Why and when does R-squared, the coefficient of determination, go below zero"><meta name=author content><link rel=canonical href=https://tnwei.github.io/posts/negative-r2/><link crossorigin=anonymous href=/assets/css/stylesheet.min.b65265c4b568a78736cd3ffdd9b9aed336896386a723b45333d8524d592f26e9.css integrity="sha256-tlJlxLVop4c2zT/92bmu0zaJY4anI7RTM9hSTVkvJuk=" rel="preload stylesheet" as=style><link rel=icon href=https://tnwei.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tnwei.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tnwei.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tnwei.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tnwei.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.104.3"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-TCJ4MGEGS8"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-TCJ4MGEGS8")</script><!doctype html><html><head><meta name=generator content="HTML Tidy for HTML5 for Linux version 5.6.0"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script>var myRenderMath=function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}]})}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=myRenderMath()></script><title></title></head><body></body></html><style>figcaption{text-align:center}</style><meta property="og:title" content="Explaining negative R-squared"><meta property="og:description" content="Why and when does R-squared, the coefficient of determination, go below zero"><meta property="og:type" content="article"><meta property="og:url" content="https://tnwei.github.io/posts/negative-r2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-06T00:36:00+08:00"><meta property="article:modified_time" content="2022-06-06T00:36:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Explaining negative R-squared"><meta name=twitter:description content="Why and when does R-squared, the coefficient of determination, go below zero"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tnwei.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Explaining negative R-squared","item":"https://tnwei.github.io/posts/negative-r2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Explaining negative R-squared","name":"Explaining negative R-squared","description":"Why and when does R-squared, the coefficient of determination, go below zero","keywords":["statistics"],"articleBody":"When I first started out doing machine learning, I learnt that:\n$R^2$ is the coefficient of determination, a measure of how well is the data explained by the fitted model, $R^2$ is the square of the coefficient of correlation, $R$, $R$ is a quantity that ranges from 0 to 1 Therefore, $R^2$ should also range from 0 to 1.\nColour me surprised when the r2_score implementation in sklearn returned negative scores. What gives?\nIf you glossed over the math by instinct, this meme is for you.\nThe answer lies in the definition $R^2$ is defined upon the basis that the total sum of squares of a fitted model is equal to the explained sum of squares plus the residual sum of squares, or:\n$$SS_{tot} = SS_{exp} + SS_{res} \\ … \\ (1)$$\nwhere:\nTotal sum of squares ($SS_{tot}$) represent the total variation in data, measured by the sum of squares of the difference between expected and actual values, Explained sum of squares ($SS_{exp}$) represent the variation in data explained by the fitted model, and Residual sum of squares ($SS_{res}$) represent variation in data that is not explained by the fitted model. $R^2$ itself is defined as follows:\n$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\ … \\ (2)$$\nGiven these definitions, note that negative $R^2$ is only possible when the residual sum of squares ($SS_{res}$) exceeds the total sum of squares ($SS_{tot}$). As this is not mathematically possible, it can only mean that the explained sum of squares and residual sum of squares no longer add up to equal the total sum of squares. In other words, the equality $SS_{tot} = SS_{exp} + SS_{res} $ does not appear 1 to be true.\nHow can this be?\nBecause we evaluate models separately on train and test data Following the above definitions, $SS_{tot}$ can be calculated using just the data itself, while $SS_{res}$ depends both on model predictions and the data. While we can use any arbitrary model to generate the predictions for scoring, we need to realize that the aforementioned equality is defined for models trained on the same data. Therefore, it doesn’t necessarily hold true when we use test data to evaluate models built on train data! There is no guarantee that the differences between a foreign model’s predictions and the data is smaller than the variation within the data itself.\nWe can demonstrate this empirically. The code below fits a couple of linear regression models on randomly generated data:\nfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score import numpy as np for _ in range(20): data = np.random.normal(size=(200, 10)) X_train = data[:160, :-1] X_test = data[160:, :-1] y_train = data[:160, -1] y_test = data[160:, -1] lr = LinearRegression() lr.fit(X_train, y_train) y_train_pred = lr.predict(X_train) y_test_pred = lr.predict(X_test) train_score = r2_score(y_train, y_train_pred) test_score = r2_score(y_test, y_test_pred) print(f\"Train R2: {train_score:.3f}, Test R2: {test_score:.3f}\") Try as we might, the $R^2$ never drops below zero when the models are evaluated on train data. Here’s what I got in STDOUT:\nTrain R2: 0.079, Test R2: -0.059 Train R2: 0.019, Test R2: -0.046 Train R2: 0.084, Test R2: -0.060 Train R2: 0.020, Test R2: -0.083 Train R2: 0.065, Test R2: -0.145 Train R2: 0.022, Test R2: 0.032 Train R2: 0.048, Test R2: 0.107 Train R2: 0.076, Test R2: -0.031 Train R2: 0.029, Test R2: 0.006 Train R2: 0.069, Test R2: -0.150 Train R2: 0.064, Test R2: -0.150 Train R2: 0.053, Test R2: 0.096 Train R2: 0.062, Test R2: 0.022 Train R2: 0.063, Test R2: 0.008 Train R2: 0.059, Test R2: -0.061 Train R2: 0.076, Test R2: -0.191 Train R2: 0.049, Test R2: 0.099 Train R2: 0.040, Test R2: -0.012 Train R2: 0.096, Test R2: -0.373 Train R2: 0.073, Test R2: 0.088 So … what about $R^2$ being the square of correlation? It appears that $R^2 = R * R$ only under limited circumstances. Quoting the paragraph below from the relevant Wikipedia page:\nThere are several definitions of $R^2$ that are only sometimes equivalent. One class of such cases includes that of simple linear regression where $r^2$ is used instead of $R^2$. When only an intercept is included, then $r^2$ is simply the square of the sample correlation coefficient (i.e., $r$) between the observed outcomes and the observed predictor values. If additional regressors are included, $R^2$ is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1.\nIn short, $R^2$ is only the square of correlation if we happen to be (1) using linear regression models, and (2) are evaluating them on the same data they are fitted (as established previously).\nOn the liberal use of $R^2$ outside the context of linear regression The quoted Wikipedia paragraph lines up with my observation flipping through statistical texts: $R^2$ is almost always introduced within the context of linear regression. That being said, the formulation of $R^2$ makes it universally defined for any arbitrary predictive model, regardless of statistical basis. It is used liberally by data scientists in regression tasks, and is even the default metric for regression models in sklearn. Is it right for us to use $R^2$ so freely outside its original context?\nHonestly, I don’t know. On one hand, it clearly has a lot of utility as a metric, which led to its widespread adoption by data scientists in the first place. On the other hand, you can find discussions like these online that caution against using $R^2$ for non-linear regression. It does seem to me that from a statistics perspective, it is important for $R^2$ to be calculated under the right conditions such that its properties can be utilized for further analysis. I take my observed relative lack of discourse about $R^2$ within data science circles to mean that from a data science perspective, $R^2$ doesn’t mean more than being a performance metric like MSE or MAE.\nPersonally, I think we are good with using $R^2$, as long as we understand it enough to know what not to do with it.\nWrapping up To summarize, we should expect $R^2$ to be bounded between zero and one only if a linear regression model is fit, and it is evaluated on the same data it is fitted on. Else, the definition of $R^2$ being $1 - \\frac{SS_{res}}{SS_{tot}}$ can lead to negative values.\nBeing specific with my choice of words here :) ↩︎\n","wordCount":"1057","inLanguage":"en","datePublished":"2022-06-06T00:36:00+08:00","dateModified":"2022-06-06T00:36:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tnwei.github.io/posts/negative-r2/"},"publisher":{"@type":"Organization","name":"Tan Nian Wei","logo":{"@type":"ImageObject","url":"https://tnwei.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://tnwei.github.io/ accesskey=h title="Tan Nian Wei (Alt + H)">Tan Nian Wei</a>
<span class=logo-switches></span></div><ul id=menu><li><a href=https://tnwei.github.io/writing/ title=Writing><span>Writing</span></a></li><li><a href="https://drive.google.com/file/d/1kWKGkamiCd7RZLssEBy3b-FT3r0jpwha/view?usp=sharing" title=Resume><span>Resume</span></a></li><li><a href=https://tnwei.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Explaining negative R-squared</h1><div class=post-meta><span title='2022-06-06 00:36:00 +0800 +0800'>Jun 6, 2022 Mon</span>&nbsp;·&nbsp;5 min</div></header><div class=post-content><p>When I first started out doing machine learning, I learnt that:</p><ul><li>$R^2$ is the coefficient of determination, a measure of how well is the data explained by the fitted model,</li><li>$R^2$ is the square of the coefficient of correlation, $R$,</li><li>$R$ is a quantity that ranges from 0 to 1</li></ul><p>Therefore, $R^2$ should also range from 0 to 1.</p><p>Colour me surprised when the <code>r2_score</code> <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html>implementation in sklearn</a> returned negative scores. What gives?</p><figure class=align-center><img loading=lazy src=/images/r2-gru.jpeg#center alt="Same info in the intro, but in Gru meme format"><figcaption><p>If you glossed over the math by instinct, this meme is for you.</p></figcaption></figure><h2 id=the-answer-lies-in-the-definition>The answer lies in the definition<a hidden class=anchor aria-hidden=true href=#the-answer-lies-in-the-definition>#</a></h2><p>$R^2$ is defined upon the basis that the total sum of squares of a fitted model is equal to the explained sum of squares plus the residual sum of squares, or:</p><p>$$SS_{tot} = SS_{exp} + SS_{res} \ &mldr; \ (1)$$</p><p>where:</p><ul><li>Total sum of squares ($SS_{tot}$) represent the total variation in data, measured by the sum of squares of the difference between expected and actual values,</li><li>Explained sum of squares ($SS_{exp}$) represent the variation in data explained by the fitted model, and</li><li>Residual sum of squares ($SS_{res}$) represent variation in data that is not explained by the fitted model.</li></ul><p>$R^2$ itself is defined as follows:</p><p>$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} \ &mldr; \ (2)$$</p><p>Given these definitions, note that negative $R^2$ is only possible when the residual sum of squares ($SS_{res}$) exceeds the total sum of squares ($SS_{tot}$). As this is not mathematically possible, it can only mean that the explained sum of squares and residual sum of squares no longer add up to equal the total sum of squares. In other words, the equality $SS_{tot} = SS_{exp} + SS_{res} $ does not appear <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> to be true.</p><p>How can this be?</p><h2 id=because-we-evaluate-models-separately-on-train-and-test-data>Because we evaluate models separately on train and test data<a hidden class=anchor aria-hidden=true href=#because-we-evaluate-models-separately-on-train-and-test-data>#</a></h2><p>Following the above definitions, $SS_{tot}$ can be calculated using just the data itself, while $SS_{res}$ depends both on model predictions and the data. While we can use any arbitrary model to generate the predictions for scoring, we need to realize that the aforementioned equality is defined for <em>models trained on the same data</em>. Therefore, it doesn&rsquo;t necessarily hold true when we use test data to evaluate models built on train data! There is no guarantee that the differences between a foreign model&rsquo;s predictions and the data is smaller than the variation within the data itself.</p><p>We can demonstrate this empirically. The code below fits a couple of linear regression models on randomly generated data:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LinearRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> r2_score
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X_train <span style=color:#f92672>=</span> data[:<span style=color:#ae81ff>160</span>, :<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    X_test <span style=color:#f92672>=</span> data[<span style=color:#ae81ff>160</span>:, :<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    y_train <span style=color:#f92672>=</span> data[:<span style=color:#ae81ff>160</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    y_test <span style=color:#f92672>=</span> data[<span style=color:#ae81ff>160</span>:, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    lr <span style=color:#f92672>=</span> LinearRegression()
</span></span><span style=display:flex><span>    lr<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    y_train_pred <span style=color:#f92672>=</span> lr<span style=color:#f92672>.</span>predict(X_train)
</span></span><span style=display:flex><span>    y_test_pred <span style=color:#f92672>=</span> lr<span style=color:#f92672>.</span>predict(X_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_score <span style=color:#f92672>=</span> r2_score(y_train, y_train_pred)
</span></span><span style=display:flex><span>    test_score <span style=color:#f92672>=</span> r2_score(y_test, y_test_pred)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Train R2: </span><span style=color:#e6db74>{</span>train_score<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, Test R2: </span><span style=color:#e6db74>{</span>test_score<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>Try as we might, the $R^2$ never drops below zero when the models are evaluated on train data. Here&rsquo;s what I got in STDOUT:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Train R2: 0.079, Test R2: -0.059
</span></span><span style=display:flex><span>Train R2: 0.019, Test R2: -0.046
</span></span><span style=display:flex><span>Train R2: 0.084, Test R2: -0.060
</span></span><span style=display:flex><span>Train R2: 0.020, Test R2: -0.083
</span></span><span style=display:flex><span>Train R2: 0.065, Test R2: -0.145
</span></span><span style=display:flex><span>Train R2: 0.022, Test R2: 0.032
</span></span><span style=display:flex><span>Train R2: 0.048, Test R2: 0.107
</span></span><span style=display:flex><span>Train R2: 0.076, Test R2: -0.031
</span></span><span style=display:flex><span>Train R2: 0.029, Test R2: 0.006
</span></span><span style=display:flex><span>Train R2: 0.069, Test R2: -0.150
</span></span><span style=display:flex><span>Train R2: 0.064, Test R2: -0.150
</span></span><span style=display:flex><span>Train R2: 0.053, Test R2: 0.096
</span></span><span style=display:flex><span>Train R2: 0.062, Test R2: 0.022
</span></span><span style=display:flex><span>Train R2: 0.063, Test R2: 0.008
</span></span><span style=display:flex><span>Train R2: 0.059, Test R2: -0.061
</span></span><span style=display:flex><span>Train R2: 0.076, Test R2: -0.191
</span></span><span style=display:flex><span>Train R2: 0.049, Test R2: 0.099
</span></span><span style=display:flex><span>Train R2: 0.040, Test R2: -0.012
</span></span><span style=display:flex><span>Train R2: 0.096, Test R2: -0.373
</span></span><span style=display:flex><span>Train R2: 0.073, Test R2: 0.088
</span></span></code></pre></div><h2 id=so--what-about-r2-being-the-square-of-correlation>So &mldr; what about $R^2$ being the square of correlation?<a hidden class=anchor aria-hidden=true href=#so--what-about-r2-being-the-square-of-correlation>#</a></h2><p>It appears that $R^2 = R * R$ only under limited circumstances. Quoting the paragraph below from the <a href=https://en.wikipedia.org/wiki/Coefficient_of_determination>relevant Wikipedia page</a>:</p><blockquote><p>There are several definitions of $R^2$ that are only sometimes equivalent. One class of such cases includes that of simple linear regression where $r^2$ is used instead of $R^2$. When only an intercept is included, then $r^2$ is simply the square of the sample correlation coefficient (i.e., $r$) between the observed outcomes and the observed predictor values. If additional regressors are included, $R^2$ is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1.</p></blockquote><p>In short, $R^2$ is only the square of correlation if we happen to be (1) using linear regression models, and (2) are evaluating them on the same data they are fitted (as established previously).</p><h2 id=on-the-liberal-use-of-r2-outside-the-context-of-linear-regression>On the liberal use of $R^2$ outside the context of linear regression<a hidden class=anchor aria-hidden=true href=#on-the-liberal-use-of-r2-outside-the-context-of-linear-regression>#</a></h2><p>The quoted Wikipedia paragraph lines up with my observation flipping through statistical texts: $R^2$ is almost always introduced within the context of linear regression. That being said, the formulation of $R^2$ makes it universally defined for any arbitrary predictive model, regardless of statistical basis. It is used liberally by data scientists in regression tasks, and is even the default metric for regression models in sklearn. Is it right for us to use $R^2$ so freely outside its original context?</p><p>Honestly, I don&rsquo;t know. On one hand, it clearly has a lot of utility as a metric, which led to its widespread adoption by data scientists in the first place. On the other hand, you can find <a href=https://stats.stackexchange.com/questions/547863/heres-why-you-can-hopefully-use-r2-for-non-linear-models-why-not>discussions</a> <a href=https://blog.minitab.com/en/adventures-in-statistics-2/why-is-there-no-r-squared-for-nonlinear-regression>like</a> <a href=https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/>these</a> online that caution against using $R^2$ for non-linear regression. It does seem to me that from a statistics perspective, it is important for $R^2$ to be calculated under the right conditions such that its properties can be utilized for further analysis. I take my observed relative lack of discourse about $R^2$ within data science circles to mean that from a data science perspective, $R^2$ doesn&rsquo;t mean more than being a performance metric like MSE or MAE.</p><p>Personally, I think we are good with using $R^2$, as long as we understand it enough to know what <em>not</em> to do with it.</p><h2 id=wrapping-up>Wrapping up<a hidden class=anchor aria-hidden=true href=#wrapping-up>#</a></h2><p>To summarize, we should expect $R^2$ to be bounded between zero and one only if a linear regression model is fit, and it is evaluated on the same data it is fitted on. Else, the definition of $R^2$ being $1 - \frac{SS_{res}}{SS_{tot}}$ can lead to negative values.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Being specific with my choice of words here :)&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://tnwei.github.io/tags/statistics/>statistics</a></li></ul></footer><script src=https://giscus.app/client.js data-repo=tnwei/tnwei.github.io data-repo-id=R_kgDOGmOfvQ data-category=Announcements data-category-id=DIC_kwDOGmOfvc4CAeit data-mapping=og:title data-reactions-enabled=0 data-emit-metadata=0 data-theme=light data-lang=en crossorigin=anonymous async></script><noscript></noscript></article></main><footer class=footer><span>&copy; 2022 <a href=https://tnwei.github.io/>Tan Nian Wei</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>